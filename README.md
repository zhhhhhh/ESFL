# Toward Efficient Defense Against Model Poisoning Attacks in Privacy-Preserving Federated Learning
This repository contains PyTorch implementation of the paper: Toward Efficient Defense Against Model Poisoning Attacks in Privacy-Preserving Federated Learning.

## Paper 

Toward Efficient Defense Against Model Poisoning Attacks in Privacy-Preserving Federated Learning

## Content
The repository contains one jupyter notebook for each benchmark which can be used to re-produce the experiments reported in the paper for that benchmark. The notebooks contain clear instructions on how to run the experiments. 

## Data sets
[MNIST](http://yann.lecun.com/exdb/mnist/) and [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) will be automatically downloaded.


## Dependencies

[Python 3.6](https://www.anaconda.com/download)

[PyTorch 1.6](https://pytorch.org/)

[TensorFlow 2](https://www.tensorflow.org/)


## Results

 The results can be seen in the paper
